{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "class FlowDataset(Dataset):\n",
    "    def __init__(self, num_points=16, data_dir='flow/', data_session='all_params_50625', mode='train'):\n",
    "        data_fn = os.path.join(data_dir, 'dataset_' + data_session + '.npy')\n",
    "        bc_fn = os.path.join(data_dir, 'bc_'+ data_session + '.npy')\n",
    "        target = np.load(data_fn)\n",
    "        inputs = np.load(bc_fn)\n",
    "\n",
    "        # Pre-Process Data\n",
    "        # replace first time step with all the boundary conditions\n",
    "        # for now, hardcoded to fill in since there are 4 bc's and 17 points\n",
    "        for i in range(len(inputs)):\n",
    "            D = inputs[i][0]\n",
    "            dpdx = inputs[i][1]\n",
    "            mu = inputs[i][2]\n",
    "            nu = inputs[i][3]\n",
    "            for j in range(num_points+1):\n",
    "                if 0 <= j < 5:\n",
    "                    target[i][0][j] = D\n",
    "                elif 5 <= j < 9:\n",
    "                    target[i][0][j] = dpdx\n",
    "                elif 9 <= j < 13:\n",
    "                    target[i][0][j] = mu\n",
    "                else:\n",
    "                    target[i][0][j] = nu\n",
    "\n",
    "        num_data = len(inputs)\n",
    "        np.random.seed(0)\n",
    "        # split the dataset inton training and test \n",
    "        test_idx = np.random.choice(num_data, num_data//5, replace=False).tolist()\n",
    "        train_idx = list(set(range(num_data)) - set(test_idx))\n",
    "\n",
    "        self.mode = mode\n",
    "        if mode is 'train':\n",
    "            self.data = target[train_idx,:,:].astype(np.float32)\n",
    "        elif mode is 'test':\n",
    "            self.data = target[test_idx,:,:].astype(np.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode is 'train':\n",
    "            return self.data[idx,:-1,:], self.data[idx,1:,:]\n",
    "        elif self.mode is 'test':\n",
    "            return self.data[idx,0,:], self.data[idx,1:,:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class FlowLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout,):\n",
    "        super(FlowLSTM, self).__init__()\n",
    "        \n",
    "        #define variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = [nn.LSTMCell(input_size=self.input_size,hidden_size=self.hidden_size)]\n",
    "        \n",
    "        for i in range(num_layers-1):\n",
    "            self.lstm += [nn.LSTMCell(input_size=self.hidden_size,hidden_size=self.hidden_size)]\n",
    "            \n",
    "        self.drop_layer = nn.Dropout(p = self.dropout)\n",
    "        self.linear = nn.Linear(self.hidden_size,self.input_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input: x of dim (batch_size, 19, 17)\n",
    "        '''\n",
    "        outps = []\n",
    "        \n",
    "        h_t = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        c_t = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        for _,sample in enumerate(x.split(1,dim=1)):\n",
    "            for i in range(self.num_layers):\n",
    "                if i ==0:\n",
    "                    h_t,c_t = self.lstm[i](sample.reshape(sample.shape[0],-1),(h_t,c_t))\n",
    "                else:\n",
    "                    h_t2,c_t2 = self.lstm[i](h_t,(h_t2,c_t2))\n",
    "            \n",
    "            outp = self.drop_layer(h_t2)\n",
    "            outp = self.linear(outp)\n",
    "            outp = outp.reshape(outp.shape[0],1,-1)\n",
    "            outps += [outp]\n",
    "        outps = torch.cat(outps,dim=1)\n",
    "        return outps\n",
    "\n",
    "    def test(self, x):\n",
    "        '''\n",
    "        input: x of dim (batch_size, 17)\n",
    "        '''\n",
    "        outps = []\n",
    "        out = x\n",
    "        \n",
    "        h_t = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        c_t = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(x.size(0),self.hidden_size,dtype=torch.float32)\n",
    "        \n",
    "        for i in range (19):\n",
    "            for i in range(self.num_layers):\n",
    "                if i ==0:\n",
    "                    h_t,c_t = self.lstm[i](out,(h_t,c_t))\n",
    "                else:\n",
    "                    h_2t,c_2t = self.lstm[i](h_t,(h_t2,c_t2))\n",
    "            out = self.linear(h_2t)\n",
    "            outps += [out.reshape(out.shape[0],1,-1)]\n",
    "        outps = torch.cat(outps,dim=1)\n",
    "\n",
    "        return outps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # check if cuda available\n",
    "    Epoch_List = []\n",
    "    Loss_List = []\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # define dataset and dataloader\n",
    "    train_dataset = FlowDataset(mode='train').data\n",
    "    test_dataset = FlowDataset(mode='test').data\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    # hyper-parameters\n",
    "    num_epochs = 20\n",
    "    lr = 0.001\n",
    "    input_size = 17 # do not change input size\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    dropout = 0.1\n",
    "\n",
    "    model = FlowLSTM(\n",
    "        input_size=input_size, \n",
    "        hidden_size=hidden_size, \n",
    "        num_layers=num_layers, \n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # define your LSTM loss function here\n",
    "    criterion = nn.MSELoss()\n",
    "    # define optimizer for lstm model\n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    #model \n",
    "    for p in model.parameters():\n",
    "        print(p.numel())\n",
    "    print(model)\n",
    "    for epoch in range(num_epochs):\n",
    "        for n_batch, data in enumerate(train_loader):\n",
    "            in_batch = data[:,:-1,:]\n",
    "            label = data[:,1:,:]\n",
    "            in_batch, label = in_batch.to(device), label.to(device)\n",
    "            out = model.forward(in_batch)\n",
    "            loss = criterion(out, label)\n",
    "            # train LSTM\n",
    "            # calculate LSTM loss\n",
    "            # loss = loss_func(...)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # print loss while training\n",
    "\n",
    "            if (n_batch + 1) % 200 == 0:\n",
    "                print(\"Epoch: [{}/{}], Batch: {}, Loss: {}\".format(\n",
    "                    epoch, num_epochs, n_batch, loss.item()))\n",
    "                if n_batch == 199:\n",
    "                    Epoch_List.append(epoch)\n",
    "                    Loss_List.append(loss.item())\n",
    "    # test trained LSTM model\n",
    "    l1_err, l2_err = 0, 0\n",
    "    l1_loss = nn.L1Loss()\n",
    "    l2_loss = nn.MSELoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for n_batch, data in enumerate(test_loader):\n",
    "            in_batch = data[:,0,:]\n",
    "            label = data[:,1:,:]\n",
    "            in_batch, label = in_batch.to(device), label.to(device)\n",
    "            pred = model.test(in_batch)\n",
    "\n",
    "            l1_err += l1_loss(pred, label).item()\n",
    "            l2_err += l2_loss(pred, label).item()\n",
    "\n",
    "    print(\"Test L1 error:\", l1_err)\n",
    "    print(\"Test L2 error:\", l2_err)\n",
    "    torch.save(model.state_dict(),'p1_model1.ckpt')\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    model_save_name = 'p1_model1.ckpt'\n",
    "    path = F\"/content/gdrive/MyDrive/{model_save_name}\"\n",
    "    torch.save(model.state_dict(),path)\n",
    "\n",
    "    # visualize the prediction comparing to the ground truth\n",
    "    if device is 'cpu':\n",
    "        pred = pred.detach().numpy()[0,:,:]\n",
    "        label = label.detach().numpy()[0,:,:]\n",
    "    else:\n",
    "        pred = pred.detach().cpu().numpy()[0,:,:]\n",
    "        label = label.detach().cpu().numpy()[0,:,:]\n",
    "\n",
    "    r = []\n",
    "    num_points = 17\n",
    "    interval = 1./num_points\n",
    "    x = int(num_points/2)\n",
    "    for j in range(-x,x+1):\n",
    "        r.append(interval*j)\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure()\n",
    "    for i in range(1, len(pred)):\n",
    "        c = (i/(num_points+1), 1-i/(num_points+1), 0.5)\n",
    "        plt.plot(pred[i], r, label='t = %s' %(i), c=c)\n",
    "    plt.xlabel('velocity [m/s]')\n",
    "    plt.ylabel('r [m]')\n",
    "    plt.legend(bbox_to_anchor=(1,1),fontsize='x-small')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(1, len(label)):\n",
    "        c = (i/(num_points+1), 1-i/(num_points+1), 0.5)\n",
    "        plt.plot(label[i], r, label='t = %s' %(i), c=c)\n",
    "    plt.xlabel('velocity [m/s]')\n",
    "    plt.ylabel('r [m]')\n",
    "    plt.legend(bbox_to_anchor=(1,1),fontsize='x-small')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(Epoch_List,Loss_List)\n",
    "    plt.title('Epoch vs Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
