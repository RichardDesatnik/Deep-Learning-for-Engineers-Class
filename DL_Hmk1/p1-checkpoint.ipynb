{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not import any additional 3rd party external libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Activation(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for activation functions (non-linearities).\n",
    "    \"\"\"\n",
    "\n",
    "    # No additional work is needed for this class, as it acts like an abstract base class for the others\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Identity(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Identity function (already implemented).\n",
    "    \"\"\"\n",
    "\n",
    "    # This class is a gimme as it is already implemented for you as an example (do not change)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Sigmoid non-linearity\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hint: save the useful data for back propagation\n",
    "        sig = 1/(1+np.exp(-1*x))\n",
    "        return sig \n",
    "\n",
    "    def derivative(self,x):\n",
    "        dsig = (1/(1+np.exp(-1*x)))*((1-(1/(1+np.exp(-1*x)))))\n",
    "        return dsig\n",
    "\n",
    "class Tanh(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Tanh non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        tanh = np.tanh(x)\n",
    "        return tanh\n",
    "\n",
    "    def derivative(self,x):\n",
    "        dtanh = 1 - (np.tanh(x)**2)\n",
    "        return dtanh\n",
    "\n",
    "class ReLU(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    ReLU non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.where(x<0,0,x)\n",
    "        return x\n",
    "\n",
    "    def derivative(self,x):\n",
    "        x = np.where(x<0,0,1)\n",
    "        return x\n",
    "\n",
    "class Criterion(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Interface for loss functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nothing needs done to this class, it's used by the following Criterion classes\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logits = None\n",
    "        self.labels = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.forward(x, y)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Criterion):\n",
    "\n",
    "    \"\"\"\n",
    "    Softmax loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SoftmaxCrossEntropy, self).__init__()\n",
    "        # you can add variables if needed\n",
    "\n",
    "    def forward(self, labels):\n",
    "        batch_size = 25\n",
    "        num_class = 10\n",
    "        CESM = np.ones([batch_size, num_class])\n",
    "        for b in range(batch_size):\n",
    "            CESM[b] = -1*np.array([labels[b]])*np.array([(np.log(np.exp(self.act[2][b])/np.sum(np.exp(labels[b]))))])\n",
    "            totloss = np.sum(CESM,axis=1)\n",
    "            Loss = np.sum(totloss)/2\n",
    "        return Loss\n",
    "\n",
    "    def derivative(self,labels,b):\n",
    "        dsoft = np.array([-labels[b]])+np.array([self.act[2][b]])\n",
    "        return dsoft   \n",
    "\n",
    "\n",
    "# randomly intialize the weight matrix with dimension d0 x d1 via Normal distribution\n",
    "def random_normal_weight_init(d0, d1):\n",
    "    return np.random.randn(d0,d1)\n",
    "\n",
    "\n",
    "# initialize a d-dimensional bias vector with all zeros\n",
    "def zeros_bias_init(d):\n",
    "    return np.zeros([1,d])\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron\n",
    "    (feel free to add class functions if needed)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hiddens, activations, weight_init_fn, bias_init_fn, criterion, lr):\n",
    "\n",
    "        # Don't change this -->\n",
    "        self.train_mode = True\n",
    "        self.nlayers = len(hiddens) + 1\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activations = activations\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        # <---------------------\n",
    "\n",
    "        # Don't change the name of the following class attributes\n",
    "        self.nn_dim = [input_size] + hiddens + [output_size]\n",
    "        # list containing Weight matrices of each layer, each should be a np.array\n",
    "        self.W = [weight_init_fn(self.nn_dim[i], self.nn_dim[i+1]) for i in range(self.nlayers)]\n",
    "        # list containing derivative of Weight matrices of each layer, each should be a np.array\n",
    "        self.dW = [np.zeros_like(weight) for weight in self.W]\n",
    "        # list containing bias vector of each layer, each should be a np.array\n",
    "        self.b = [bias_init_fn(self.nn_dim[i+1]) for i in range(self.nlayers)]\n",
    "        # list containing derivative of bias vector of each layer, each should be a np.array\n",
    "        self.db = [np.zeros_like(bias) for bias in self.b]\n",
    "\n",
    "        # You can add more variables if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = 25\n",
    "        r0,c0 = np.shape(self.W[0])\n",
    "        r1,c1 = np.shape(self.W[1])\n",
    "        r2,c2 = np.shape(self.W[2])\n",
    "        self.act = [np.ones([25,c0]),np.ones([25,c1]),np.ones([25,c2])]\n",
    "        for b in range(batchsize):\n",
    "            layers = len(self.nn_dim)\n",
    "            for i in range(layers-1):\n",
    "                if i == 0:\n",
    "                    f = np.dot(np.array([x[b]]) , self.W[i]) + self.b[i]\n",
    "                    self.act[0][b] = np.array([self.activations[i].forward(f)])\n",
    "                else:\n",
    "                    f = np.dot(self.act[i-1][b] , self.W[i])+self.b[i]\n",
    "                    self.act[i][b] = np.array([self.activations[i-1].forward(f)])\n",
    "        return self.act\n",
    "\n",
    "    def zero_grads(self):\n",
    "        layers = len(self.nn_dim)\n",
    "        for i in range(layers-1):\n",
    "            Wn, WM = np.shape(self.dW[i])\n",
    "            bn, bm = np.shape(self.b[i])\n",
    "            self.dW[i] = np.zeros([Wn,Wm])\n",
    "            self.db[i] = np.zeros([bn,bm])\n",
    "        return self.dW,self.db\n",
    "\n",
    "    def step(self):     \n",
    "        # update the W and b on each layer\n",
    "        layers = len(self.nn_dim)\n",
    "        for i in range(layers-1):\n",
    "            self.W[i] = self.W[i] - self.lr*self.dW[i] \n",
    "            self.b[i] = self.b[i] - self.lr*self.db[i] \n",
    "        return self.W[i],self.b[i]\n",
    "\n",
    "    def backward(self, labels, x):\n",
    "        batchsize = 25\n",
    "        layers = len(self.nn_dim)\n",
    "    \n",
    "        for b in range(batchsize):\n",
    "            \n",
    "            if self.train_mode:\n",
    "                \n",
    "                \n",
    "              \n",
    "                for i in range(layers-1):\n",
    "                    #print(SoftmaxCrossEntropy.derivative(self, labels, b))\n",
    "                    if i == 0:\n",
    "                        #print(SoftmaxCrossEntropy.derivative(self, labels, b) == (np.array([-labels[b]])+np.array([self.act[2][b]])))\n",
    "                        #self.dW[0] = np.sum(((np.array([-labels[b]])+np.array([self.act[2][b]]))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))@(self.W[1].T)*self.activations[0].derivative(np.array([self.act[0][b]]))*(np.array(x[i][b]).T)\n",
    "                        #self.db[0] = np.sum(((np.array([-labels[b]])+np.array([self.act[2][b]]))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))@(self.W[1].T)*self.activations[0].derivative(np.array([self.act[0][b]]))\n",
    "                        self.dW[0] = np.sum(((SoftmaxCrossEntropy.derivative(self, labels, b))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))@(self.W[1].T)*self.activations[0].derivative(np.array([self.act[0][b]]))*(np.array(x[i][b]).T)\n",
    "                        self.db[0] = np.sum(((SoftmaxCrossEntropy.derivative(self, labels, b))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))@(self.W[1].T)*self.activations[0].derivative(np.array([self.act[0][b]]))\n",
    "                    elif i == 1:\n",
    "                        #print(SoftmaxCrossEntropy.derivative(self, labels, b) == (np.array([-labels[b]])+np.array([self.act[2][b]])))\n",
    "                        #self.dW[1] = np.sum(((np.array([-labels[b]])+np.array([self.act[2][b]]))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))*((np.array([self.act[0][b]])).T)      \n",
    "                        #self.db[1] = np.sum(((np.array([-labels[b]])+np.array([self.act[2][b]]))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))\n",
    "                        self.dW[1] = np.sum(((SoftmaxCrossEntropy.derivative(self, labels, b))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))*((np.array([self.act[0][b]])).T)      \n",
    "                        self.db[1] = np.sum(((SoftmaxCrossEntropy.derivative(self, labels, b))@(self.W[2].T)),axis=0)*self.activations[1].derivative(np.array([self.act[1][b]]))\n",
    "                    elif i == 2:\n",
    "                        #print(SoftmaxCrossEntropy.derivative(self, labels, b) == (np.array([-labels[b]])+np.array([self.act[2][b]])))\n",
    "                        #self.dW[2] = ((np.array([-labels[b]])+np.array([self.act[2][b]])).T@np.array([self.act[1][b]])).T\n",
    "                        #self.db[2] = (np.array([-labels[b]])+np.array([self.act[2][b]]))\n",
    "                        self.dW[2] = ((SoftmaxCrossEntropy.derivative(self, labels, b)).T@np.array([self.act[1][b]])).T\n",
    "                        self.db[2] = (SoftmaxCrossEntropy.derivative(self, labels, b))\n",
    "                        \n",
    "            return self.dW, self.db\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        # training mode\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        # evaluation mode\n",
    "        self.train_mode = False\n",
    "\n",
    "    def get_loss(self, labels):\n",
    "        Loss = SoftmaxCrossEntropy.forward(self, labels)\n",
    "        return Loss\n",
    "\n",
    "    def get_error(self, labels):\n",
    "        batch_size = 25\n",
    "        correct_list = []\n",
    "        wrong_list = []\n",
    "        for b in range(batch_size):\n",
    "            lab_ans = np.argmax(labels[b])\n",
    "            my_ans = np.argmax(self.act[2][b])\n",
    "            if lab_ans == my_ans:\n",
    "                #print(\"Correct\")\n",
    "                correct_list.append(1)\n",
    "            else:\n",
    "                #print(\"wrong\")\n",
    "                wrong_list.append(1)\n",
    "\n",
    "        acc = len(correct_list)/(len(wrong_list)+len(correct_list))\n",
    "        Num_Incorrect = len(wrong_list)\n",
    "        return Num_Incorrect \n",
    "\n",
    "    def save_model(self, path='p1_model.npz'):\n",
    "        # save the parameters of MLP (do not change)\n",
    "        np.savez(path, self.W[0], self.b[0])\n",
    "\n",
    "\n",
    "# Don't change this function\n",
    "def get_training_stats(mlp, dset, nepochs, batch_size):\n",
    "    train, val, test = dset\n",
    "    trainx, trainy = train\n",
    "    valx, valy = val\n",
    "    testx, testy = test\n",
    "\n",
    "    idxs = np.arange(len(trainx))\n",
    "\n",
    "    training_losses = []\n",
    "    training_errors = []\n",
    "    validation_losses = []\n",
    "    validation_errors = []\n",
    "\n",
    "    for e in range(nepochs):\n",
    "        print(\"epoch: \", e)\n",
    "        train_loss = 0\n",
    "        train_error = 0\n",
    "        val_loss = 0\n",
    "        val_error = 0\n",
    "        num_train = len(trainx)\n",
    "        num_val = len(valx)\n",
    "\n",
    "        for b in range(0, num_train, batch_size):\n",
    "            mlp.train()\n",
    "            mlp(trainx[b:b+batch_size])\n",
    "            x = trainx[b:b+batch_size]\n",
    "            mlp.backward(trainy[b:b+batch_size],x)\n",
    "            mlp.step()\n",
    "            train_loss += mlp.get_loss(trainy[b:b+batch_size])\n",
    "            train_error += mlp.get_error(trainy[b:b+batch_size])\n",
    "        training_losses += [train_loss/num_train]\n",
    "        training_errors += [train_error/num_train]\n",
    "        print(\"training loss: \", train_loss/num_train)\n",
    "        print(\"training error: \", train_error/num_train)\n",
    "        \n",
    "        for b in range(0, num_val, batch_size):\n",
    "            mlp.eval()\n",
    "            mlp(valx[b:b+batch_size])\n",
    "            val_loss += mlp.get_loss(valy[b:b+batch_size])\n",
    "            val_error += mlp.get_error(valy[b:b+batch_size])\n",
    "        validation_losses += [val_loss/num_val]\n",
    "        validation_errors += [val_error/num_val]\n",
    "        print(\"validation loss: \", val_loss/num_val)\n",
    "        print(\"validation error: \", val_error/num_val)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_error = 0\n",
    "    num_test = len(testx)\n",
    "    for b in range(0, num_test, batch_size):\n",
    "        mlp.eval()\n",
    "        mlp(testx[b:b+batch_size])\n",
    "        test_loss += mlp.get_loss(testy[b:b+batch_size])\n",
    "        test_error += mlp.get_error(testy[b:b+batch_size])\n",
    "    test_loss /= num_test\n",
    "    test_error /= num_test\n",
    "    print(\"test loss: \", test_loss)\n",
    "    print(\"test error: \", test_error)\n",
    "\n",
    "    return (training_losses, training_errors, validation_losses, validation_errors)\n",
    "\n",
    "\n",
    "# get ont hot key encoding of the label (no need to change this function)\n",
    "def get_one_hot(in_array, one_hot_dim):\n",
    "    dim = in_array.shape[0]\n",
    "    out_array = np.zeros((dim, one_hot_dim))\n",
    "    for i in range(dim):\n",
    "        idx = int(in_array[i])\n",
    "        out_array[i, idx] = 1\n",
    "    return out_array\n",
    "\n",
    "\n",
    "def main(lr=.05,num_epochs=100,hiddens=[6000,2000]):\n",
    "    # load the mnist dataset from csv files\n",
    "    image_size = 28 # width and length of mnist image\n",
    "    num_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "    image_pixels = image_size * image_size\n",
    "    data_path = \"mnist/\"\n",
    "    train_data = np.loadtxt(data_path + \"mnist_train.csv\", delimiter=\",\")\n",
    "    test_data = np.loadtxt(data_path + \"mnist_test.csv\", delimiter=\",\")\n",
    "\n",
    "    # rescale image from 0-255 to 0-1\n",
    "    fac = 1.0 / 255\n",
    "    train_imgs = np.asfarray(train_data[:50000, 1:]) * fac\n",
    "    val_imgs = np.asfarray(train_data[50000:, 1:]) * fac\n",
    "    test_imgs = np.asfarray(test_data[:, 1:]) * fac\n",
    "    train_labels = np.asfarray(train_data[:50000, :1])\n",
    "    val_labels = np.asfarray(train_data[50000:, :1])\n",
    "    test_labels = np.asfarray(test_data[:, :1])\n",
    "\n",
    "    # convert labels to one-hot-key encoding\n",
    "    train_labels = get_one_hot(train_labels, num_labels)\n",
    "    val_labels = get_one_hot(val_labels, num_labels)\n",
    "    test_labels = get_one_hot(test_labels, num_labels)\n",
    "\n",
    "    print(train_imgs.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(val_imgs.shape)\n",
    "    print(val_labels.shape)\n",
    "    print(test_imgs.shape)\n",
    "    print(test_labels.shape)\n",
    "\n",
    "    dataset = [\n",
    "        [train_imgs, train_labels],\n",
    "        [val_imgs, val_labels],\n",
    "        [test_imgs, test_labels]\n",
    "    ]\n",
    "\n",
    "    # These are only examples of parameters you can start with\n",
    "    # you can tune these parameters to improve the performance of your MLP\n",
    "    # this is the only part you need to change in main() function\n",
    "    hiddens = hiddens\n",
    "    activations = [Sigmoid(), Sigmoid(), Sigmoid()]\n",
    "    lr = lr\n",
    "    #print(\"Learning rate is: \" + str(lr))\n",
    "    num_epochs = num_epochs\n",
    "    batch_size = 25\n",
    "\n",
    "    # build your MLP model\n",
    "    mlp = MLP(\n",
    "        input_size=image_pixels, \n",
    "        output_size=num_labels, \n",
    "        hiddens=hiddens, \n",
    "        activations=activations, \n",
    "        weight_init_fn=random_normal_weight_init, \n",
    "        bias_init_fn=zeros_bias_init, \n",
    "        criterion=SoftmaxCrossEntropy(), \n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    # train the neural network\n",
    "    losses = get_training_stats(mlp, dataset, num_epochs, batch_size)\n",
    "\n",
    "    # save the parameters\n",
    "    mlp.save_model()\n",
    "\n",
    "    # visualize the training and validation loss with epochs\n",
    "    training_losses, training_errors, validation_losses, validation_errors = losses\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    ax1.plot(training_losses, color='blue', label=\"training\")\n",
    "    ax1.plot(validation_losses, color='red', label='validation')\n",
    "    ax1.set_title('Loss during training')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(training_errors, color='blue', label=\"training\")\n",
    "    ax2.plot(validation_errors, color='red', label=\"validation\")\n",
    "    ax2.set_title('Error during training')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('error')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
